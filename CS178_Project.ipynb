{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "541dc45e-1057-4dfc-8de4-45f745a448eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 25000\n",
      "Test samples : 25000\n",
      "Train: 20000\n",
      "Val  : 5000\n",
      "Test : 25000\n"
     ]
    }
   ],
   "source": [
    "# shared_setup.py\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# --------- 1. Load IMDB train/test ----------\n",
    "train_path = \"aclImdb/train\"\n",
    "test_path  = \"aclImdb/test\"\n",
    "\n",
    "train_data = load_files(\n",
    "    train_path,\n",
    "    categories=['pos', 'neg'],\n",
    "    encoding='utf-8'\n",
    ")\n",
    "test_data = load_files(\n",
    "    test_path,\n",
    "    categories=['pos', 'neg'],\n",
    "    encoding='utf-8'\n",
    ")\n",
    "\n",
    "X_all = train_data.data\n",
    "y_all = train_data.target\n",
    "X_test_text = test_data.data\n",
    "y_test = test_data.target\n",
    "\n",
    "print(\"Train samples:\", len(X_all))\n",
    "print(\"Test samples :\", len(X_test_text))\n",
    "\n",
    "# --------- 2. Split into train / validation ----------\n",
    "X_train_text, X_val_text, y_train, y_val = train_test_split(\n",
    "    X_all, y_all,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y_all\n",
    ")\n",
    "\n",
    "print(\"Train:\", len(X_train_text))\n",
    "print(\"Val  :\", len(X_val_text))\n",
    "print(\"Test :\", len(X_test_text))\n",
    "\n",
    "def report_errors(name, y_true, y_pred):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    err = 1.0 - acc\n",
    "    print(f\"{name} accuracy: {acc:.4f} | error rate: {err:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f8e5fcd6-c2d0-4d98-b641-dc0231d6360f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n",
      "Best params: {'clf__C': 10.0, 'tfidf__max_features': 20000, 'tfidf__ngram_range': (1, 2)}\n",
      "Best CV accuracy: 0.8916998119490964\n",
      "\n",
      "=== Logistic Regression with TF–IDF + n-grams ===\n",
      "Train accuracy: 0.9916 | error rate: 0.0084\n",
      "Val   accuracy: 0.9022 | error rate: 0.0978\n",
      "Test  accuracy: 0.8940 | error rate: 0.1060\n"
     ]
    }
   ],
   "source": [
    "# logistic_tfidf_grid.py\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# from shared_setup import X_train_text, X_val_text, X_test_text, y_train, y_val, y_test, report_errors\n",
    "\n",
    "# ---------- Pipeline: TF–IDF + LogisticRegression ----------\n",
    "logreg_pipeline = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer()),\n",
    "    (\"clf\", LogisticRegression(max_iter=1000, solver=\"liblinear\"))\n",
    "])\n",
    "\n",
    "# ---------- Grid of hyperparameters ----------\n",
    "param_grid = {\n",
    "    \"tfidf__ngram_range\": [(1,1), (1,2)],      # unigrams vs uni+bi\n",
    "    \"tfidf__max_features\": [10000, 20000],\n",
    "    \"clf__C\": [0.1, 1.0, 10.0],                # regularization strength\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    logreg_pipeline,\n",
    "    param_grid=param_grid,\n",
    "    cv=3,\n",
    "    n_jobs=-1,\n",
    "    scoring=\"accuracy\",\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ---------- Fit grid search only on TRAIN ----------\n",
    "grid.fit(X_train_text, y_train)\n",
    "\n",
    "print(\"Best params:\", grid.best_params_)\n",
    "print(\"Best CV accuracy:\", grid.best_score_)\n",
    "\n",
    "best_logreg = grid.best_estimator_\n",
    "\n",
    "# ---------- Evaluate on TRAIN / VAL / TEST ----------\n",
    "y_train_pred = best_logreg.predict(X_train_text)\n",
    "y_val_pred   = best_logreg.predict(X_val_text)\n",
    "y_test_pred  = best_logreg.predict(X_test_text)\n",
    "\n",
    "print(\"\\n=== Logistic Regression with TF–IDF + n-grams ===\")\n",
    "report_errors(\"Train\", y_train, y_train_pred)\n",
    "report_errors(\"Val  \", y_val,   y_val_pred)\n",
    "report_errors(\"Test \", y_test,  y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2518ce75-e791-4e28-82ce-8d28978159f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# svm_tfidf_grid.py\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# from shared_setup import X_train_text, X_val_text, X_test_text, y_train, y_val, y_test, report_errors\n",
    "\n",
    "svm_pipeline = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer()),\n",
    "    (\"clf\", LinearSVC())   # linear SVM for text\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    \"tfidf__ngram_range\": [(1,1), (1,2)],\n",
    "    \"tfidf__max_features\": [10000, 20000],\n",
    "    \"clf__C\": [0.01, 0.1, 1.0, 10.0],\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    svm_pipeline,\n",
    "    param_grid=param_grid,\n",
    "    cv=3,\n",
    "    n_jobs=-1,\n",
    "    scoring=\"accuracy\",\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid.fit(X_train_text, y_train)\n",
    "\n",
    "print(\"Best params:\", grid.best_params_)\n",
    "print(\"Best CV accuracy:\", grid.best_score_)\n",
    "\n",
    "best_svm = grid.best_estimator_\n",
    "\n",
    "y_train_pred = best_svm.predict(X_train_text)\n",
    "y_val_pred   = best_svm.predict(X_val_text)\n",
    "y_test_pred  = best_svm.predict(X_test_text)\n",
    "\n",
    "print(\"\\n=== Linear SVM with TF–IDF ===\")\n",
    "report_errors(\"Train\", y_train, y_train_pred)\n",
    "report_errors(\"Val  \", y_val,   y_val_pred)\n",
    "report_errors(\"Test \", y_test,  y_test_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b43840-2279-41c3-b5c6-eb7c152f99ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bilstm_imdb_grid.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# from shared_setup import X_train_text, X_val_text, X_test_text, y_train, y_val, y_test, report_errors\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ---------- 1. Simple tokenizer ----------\n",
    "def simple_tokenize(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z0-9]+\", \" \", text)\n",
    "    return text.strip().split()\n",
    "\n",
    "# ---------- 2. Build vocabulary ----------\n",
    "def build_vocab(texts, min_freq=5, max_size=20000):\n",
    "    counter = Counter()\n",
    "    for t in texts:\n",
    "        counter.update(simple_tokenize(t))\n",
    "    # Reserve 0 for PAD, 1 for UNK\n",
    "    vocab = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "    for word, freq in counter.most_common():\n",
    "        if freq < min_freq:\n",
    "            break\n",
    "        if len(vocab) >= max_size:\n",
    "            break\n",
    "        vocab[word] = len(vocab)\n",
    "    return vocab\n",
    "\n",
    "vocab = build_vocab(X_train_text)\n",
    "vocab_size = len(vocab)\n",
    "print(\"Vocab size:\", vocab_size)\n",
    "\n",
    "def encode(text, vocab, max_len=300):\n",
    "    tokens = simple_tokenize(text)\n",
    "    ids = [vocab.get(tok, vocab[\"<UNK>\"]) for tok in tokens][:max_len]\n",
    "    if len(ids) < max_len:\n",
    "        ids += [vocab[\"<PAD>\"]] * (max_len - len(ids))\n",
    "    return ids\n",
    "\n",
    "# ---------- 3. Dataset ----------\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, vocab, max_len=300):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.vocab = vocab\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ids = torch.tensor(\n",
    "            encode(self.texts[idx], self.vocab, self.max_len),\n",
    "            dtype=torch.long\n",
    "        )\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return ids, label\n",
    "\n",
    "train_dataset = TextDataset(X_train_text, y_train, vocab)\n",
    "val_dataset   = TextDataset(X_val_text,   y_val,   vocab)\n",
    "test_dataset  = TextDataset(X_test_text,  y_test,  vocab)\n",
    "\n",
    "# ---------- 4. BiLSTM model ----------\n",
    "class BiLSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers, dropout, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(\n",
    "            embed_dim,\n",
    "            hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            bidirectional=True,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0.0,\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        emb = self.embedding(x)          # (B, T, E)\n",
    "        out, (h_n, c_n) = self.lstm(emb) # h_n: (num_layers*2, B, H)\n",
    "        # Concatenate last forward and backward hidden states\n",
    "        h_forward = h_n[-2,:,:]\n",
    "        h_backward = h_n[-1,:,:]\n",
    "        h = torch.cat([h_forward, h_backward], dim=1)  # (B, 2H)\n",
    "        h = self.dropout(h)\n",
    "        logits = self.fc(h)\n",
    "        return logits\n",
    "\n",
    "# ---------- 5. Training / eval loops ----------\n",
    "def train_one_model(params):\n",
    "    print(\"\\nTraining BiLSTM with params:\", params)\n",
    "    model = BiLSTMClassifier(\n",
    "        vocab_size=vocab_size,\n",
    "        embed_dim=params[\"embed_dim\"],\n",
    "        hidden_dim=params[\"hidden_dim\"],\n",
    "        num_layers=params[\"num_layers\"],\n",
    "        dropout=params[\"dropout\"],\n",
    "        num_classes=2\n",
    "    ).to(device)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=params[\"batch_size\"], shuffle=True)\n",
    "    val_loader   = DataLoader(val_dataset,   batch_size=params[\"batch_size\"])\n",
    "    test_loader  = DataLoader(test_dataset,  batch_size=params[\"batch_size\"])\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=params[\"lr\"])\n",
    "\n",
    "    best_val_acc = 0.0\n",
    "    best_state = None\n",
    "\n",
    "    for epoch in range(params[\"epochs\"]):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(X_batch)\n",
    "            loss = criterion(logits, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item() * X_batch.size(0)\n",
    "\n",
    "        # validation\n",
    "        model.eval()\n",
    "        y_val_all, y_val_pred_all = [], []\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch = X_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "                logits = model(X_batch)\n",
    "                preds = torch.argmax(logits, dim=1)\n",
    "                y_val_all.append(y_batch.cpu())\n",
    "                y_val_pred_all.append(preds.cpu())\n",
    "        y_val_all = torch.cat(y_val_all).numpy()\n",
    "        y_val_pred_all = torch.cat(y_val_pred_all).numpy()\n",
    "        val_acc = accuracy_score(y_val_all, y_val_pred_all)\n",
    "        print(f\"Epoch {epoch+1}: val_acc={val_acc:.4f}\")\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_state = model.state_dict()\n",
    "\n",
    "    # load best state\n",
    "    model.load_state_dict(best_state)\n",
    "\n",
    "    # compute train/val/test errors\n",
    "    def eval_loader(loader, y_true_full):\n",
    "        model.eval()\n",
    "        y_pred_all = []\n",
    "        with torch.no_grad():\n",
    "            for X_batch, _ in loader:\n",
    "                X_batch = X_batch.to(device)\n",
    "                logits = model(X_batch)\n",
    "                preds = torch.argmax(logits, dim=1)\n",
    "                y_pred_all.append(preds.cpu())\n",
    "        y_pred_all = torch.cat(y_pred_all).numpy()\n",
    "        return y_pred_all\n",
    "\n",
    "    train_loader_eval = DataLoader(train_dataset, batch_size=params[\"batch_size\"])\n",
    "    val_loader_eval   = DataLoader(val_dataset,   batch_size=params[\"batch_size\"])\n",
    "    test_loader_eval  = DataLoader(test_dataset,  batch_size=params[\"batch_size\"])\n",
    "\n",
    "    y_train_pred = eval_loader(train_loader_eval, y_train)\n",
    "    y_val_pred   = eval_loader(val_loader_eval,   y_val)\n",
    "    y_test_pred  = eval_loader(test_loader_eval,  y_test)\n",
    "\n",
    "    print(\"\\n=== BiLSTM Results ===\")\n",
    "    report_errors(\"Train\", y_train, y_train_pred)\n",
    "    report_errors(\"Val  \", y_val,   y_val_pred)\n",
    "    report_errors(\"Test \", y_test,  y_test_pred)\n",
    "\n",
    "    return best_val_acc\n",
    "\n",
    "# ---------- 6. Manual \"grid search\" ----------\n",
    "param_grid = {\n",
    "    \"embed_dim\":  [100],\n",
    "    \"hidden_dim\": [128, 256],\n",
    "    \"num_layers\": [1, 2],\n",
    "    \"dropout\":    [0.3, 0.5],\n",
    "    \"batch_size\": [64],\n",
    "    \"lr\":         [1e-3],\n",
    "    \"epochs\":     [5],   # bump if you have time/GPU\n",
    "}\n",
    "\n",
    "best_overall_val = 0.0\n",
    "best_params = None\n",
    "\n",
    "for params in ParameterGrid(param_grid):\n",
    "    val_acc = train_one_model(params)\n",
    "    if val_acc > best_overall_val:\n",
    "        best_overall_val = val_acc\n",
    "        best_params = params\n",
    "\n",
    "print(\"\\nBest BiLSTM config:\", best_params)\n",
    "print(\"Best BiLSTM val acc:\", best_overall_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9314cee4-34f8-4827-9a49-6556213a4a44",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# distilbert_imdb_grid.py\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (\n",
    "    DistilBertTokenizerFast,\n",
    "    DistilBertForSequenceClassification,\n",
    "    Trainer, TrainingArguments\n",
    ")\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# from shared_setup import X_train_text, X_val_text, X_test_text, y_train, y_val, y_test, report_errors\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(model_name)\n",
    "\n",
    "max_length = 256\n",
    "\n",
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=256):\n",
    "        self.encodings = tokenizer(\n",
    "            texts,\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=max_length\n",
    "        )\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "train_dataset = IMDBDataset(X_train_text, y_train, tokenizer, max_length)\n",
    "val_dataset   = IMDBDataset(X_val_text,   y_val,   tokenizer, max_length)\n",
    "test_dataset  = IMDBDataset(X_test_text,  y_test,  tokenizer, max_length)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = logits.argmax(axis=-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"accuracy\": acc}\n",
    "\n",
    "def run_one_config(params):\n",
    "    print(\"\\nFine-tuning DistilBERT with params:\", params)\n",
    "    model = DistilBertForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=2\n",
    "    )\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./distilbert_imdb\",\n",
    "        num_train_epochs=params[\"epochs\"],\n",
    "        per_device_train_batch_size=params[\"batch_size\"],\n",
    "        per_device_eval_batch_size=params[\"batch_size\"],\n",
    "        learning_rate=params[\"lr\"],\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"no\",\n",
    "        logging_steps=100,\n",
    "        load_best_model_at_end=False,\n",
    "        report_to=[],\n",
    "        no_cuda=(not torch.cuda.is_available()),\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    # Eval on train/val/test\n",
    "    train_metrics = trainer.evaluate(train_dataset)\n",
    "    val_metrics   = trainer.evaluate(val_dataset)\n",
    "    test_metrics  = trainer.evaluate(test_dataset)\n",
    "\n",
    "    print(\"\\n=== DistilBERT Results ===\")\n",
    "    report_errors(\"Train\", y_train, trainer.predict(train_dataset).predictions.argmax(axis=-1))\n",
    "    report_errors(\"Val  \", y_val,   trainer.predict(val_dataset).predictions.argmax(axis=-1))\n",
    "    report_errors(\"Test \", y_test,  trainer.predict(test_dataset).predictions.argmax(axis=-1))\n",
    "\n",
    "    return val_metrics[\"eval_accuracy\"]\n",
    "\n",
    "param_grid = {\n",
    "    \"lr\":      [2e-5, 5e-5],\n",
    "    \"batch_size\": [16],\n",
    "    \"epochs\":  [2, 3],\n",
    "}\n",
    "\n",
    "best_val = 0.0\n",
    "best_params = None\n",
    "\n",
    "for params in ParameterGrid(param_grid):\n",
    "    val_acc = run_one_config(params)\n",
    "    if val_acc > best_val:\n",
    "        best_val = val_acc\n",
    "        best_params = params\n",
    "\n",
    "print(\"\\nBest DistilBERT config:\", best_params)\n",
    "print(\"Best DistilBERT val acc:\", best_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4d4ccc86-5d15-48fc-926c-9b5bbb09a7ce",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded train samples: 25000\n",
      "Loaded test samples : 25000\n",
      "Training samples: 20000\n",
      "Validation samples: 5000\n",
      "Train matrix shape: (20000, 20000)\n",
      "\n",
      "=== Validation Performance (for model selection) ===\n",
      "Validation Accuracy: 89.12 %\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.89      0.89      0.89      2500\n",
      "         pos       0.89      0.90      0.89      2500\n",
      "\n",
      "    accuracy                           0.89      5000\n",
      "   macro avg       0.89      0.89      0.89      5000\n",
      "weighted avg       0.89      0.89      0.89      5000\n",
      "\n",
      "\n",
      "Retraining final model on full training data (train + val)...\n",
      "Full training samples: 25000\n",
      "\n",
      "=== FINAL TEST PERFORMANCE (unseen data) ===\n",
      "Test Accuracy: 88.37 %\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.88      0.88      0.88     12500\n",
      "         pos       0.88      0.88      0.88     12500\n",
      "\n",
      "    accuracy                           0.88     25000\n",
      "   macro avg       0.88      0.88      0.88     25000\n",
      "weighted avg       0.88      0.88      0.88     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 1. Load TRAIN and TEST sets\n",
    "# ---------------------------------------------------------------\n",
    "personal_path = \"/Users/jensonphan/git-test\"\n",
    "train_path = f\"{personal_path}/aclImdb/train\"\n",
    "test_path  = f\"{personal_path}/aclImdb/test\"\n",
    "\n",
    "train_data = load_files( train_path, categories=['pos', 'neg'], encoding='utf-8' )\n",
    "test_data = load_files( test_path, categories=['pos', 'neg'], encoding='utf-8' )\n",
    "# Train\n",
    "X_text_all = train_data.data\n",
    "y_all       = train_data.target\n",
    "# Test\n",
    "X_test_text = test_data.data\n",
    "y_test      = test_data.target\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 2. Split TRAIN into train/validation for model selection\n",
    "# ---------------------------------------------------------------\n",
    "X_train_text, X_val_text, y_train, y_val = train_test_split(\n",
    "    X_text_all, y_all,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y_all\n",
    ")\n",
    "\n",
    "print(\"Training samples:\", len(X_train_text))\n",
    "print(\"Validation samples:\", len(X_val_text))\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 3. Vectorizer (fit ONLY on training part during model selection)\n",
    "# ---------------------------------------------------------------\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=20000,\n",
    "    ngram_range=(1, 1)  # unigrams only; you can change this later\n",
    ")\n",
    "\n",
    "X_train = vectorizer.fit_transform(X_train_text)\n",
    "X_val   = vectorizer.transform(X_val_text)\n",
    "\n",
    "print(\"Train matrix shape:\", X_train.shape)\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 4. Model selection phase: train + validate\n",
    "#    (Here we just use a single config; you could GridSearch instead)\n",
    "# ---------------------------------------------------------------\n",
    "model = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    C=1.0,\n",
    "    solver='liblinear'\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_val_pred = model.predict(X_val)\n",
    "val_acc = accuracy_score(y_val, y_val_pred)\n",
    "print(\"\\n=== Validation Performance (for model selection) ===\")\n",
    "print(\"Validation Accuracy:\", round(val_acc * 100, 2), \"%\")\n",
    "print(classification_report(y_val, y_val_pred, target_names=['neg', 'pos']))\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "# >>> At this point, you would normally:\n",
    "#     - compare different C, ngram_range, etc.\n",
    "#     - pick the best config based on validation scores.\n",
    "#     We’ll assume the current config is the chosen one.\n",
    "# ----------------------------------------------------------------\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 5. FINAL TRAINING: retrain on FULL TRAINING DATA (train + val)\n",
    "# ---------------------------------------------------------------\n",
    "\n",
    "# Combine train + val text and labels\n",
    "X_full_text = X_train_text + X_val_text   # lists of strings can be concatenated\n",
    "y_full      = np.concatenate([y_train, y_val])\n",
    "\n",
    "print(\"\\nRetraining final model on full training data (train + val)...\")\n",
    "print(\"Full training samples:\", len(X_full_text))\n",
    "\n",
    "# Refit vectorizer on ALL training text\n",
    "# (this lets it learn from all available labeled data)\n",
    "vectorizer_final = TfidfVectorizer(\n",
    "    max_features=20000,\n",
    "    ngram_range=(1, 1)\n",
    ")\n",
    "X_full = vectorizer_final.fit_transform(X_full_text)\n",
    "\n",
    "# New model instance (same hyperparameters as chosen above)\n",
    "final_model = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    C=1.0,\n",
    "    solver='liblinear'\n",
    ")\n",
    "\n",
    "final_model.fit(X_full, y_full)\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 6. FINAL EVALUATION ON HELD-OUT TEST SET\n",
    "# ---------------------------------------------------------------\n",
    "# Transform test data with the FINAL vectorizer\n",
    "X_test = vectorizer_final.transform(X_test_text)\n",
    "\n",
    "y_test_pred = final_model.predict(X_test)\n",
    "test_acc = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "print(\"\\n=== FINAL TEST PERFORMANCE (unseen data) ===\")\n",
    "print(\"Test Accuracy:\", round(test_acc * 100, 2), \"%\")\n",
    "print(classification_report(y_test, y_test_pred, target_names=['neg', 'pos']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "37f84484-425c-4361-8488-19bf169d2f10",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 25000 training samples.\n",
      "Loaded 25000 testing samples.\n",
      "\\Testing Accuracy: 92.89 %\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.92      0.93     12500\n",
      "           1       0.93      0.93      0.93     12500\n",
      "\n",
      "    accuracy                           0.93     25000\n",
      "   macro avg       0.93      0.93      0.93     25000\n",
      "weighted avg       0.93      0.93      0.93     25000\n",
      "\n",
      "\\Training Accuracy: 92.89 %\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.92      0.93     12500\n",
      "           1       0.93      0.93      0.93     12500\n",
      "\n",
      "    accuracy                           0.93     25000\n",
      "   macro avg       0.93      0.93      0.93     25000\n",
      "weighted avg       0.93      0.93      0.93     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "dataset_path_train = \"/Users/jensonphan/git-test/aclImdb/train\"  # adjust for your environment\n",
    "data_train = load_files(dataset_path_train, categories=['pos', 'neg'], encoding='utf-8')\n",
    "X_text_train = data.data; y_train = data_train.target\n",
    "print(\"Loaded\", len(X_text_train), \"training samples.\")\n",
    "\n",
    "dataset_path_test = \"/Users/jensonphan/git-test/aclImdb/test\"  # adjust for your environment\n",
    "data_test = load_files(dataset_path_test, categories=['pos', 'neg'], encoding='utf-8')\n",
    "X_text_test = data.data; y_test = data.target\n",
    "print(\"Loaded\", len(X_text_test), \"testing samples.\")\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=20000, ngram_range=(1,1))\n",
    "X_train_vec = vectorizer.fit_transform(X_text_train)\n",
    "X_test_vec = vectorizer.transform(X_text_test)\n",
    "model = LogisticRegression( max_iter=1000, C=1.0, solver='liblinear')\n",
    "model.fit(X_train_vec, y_train)\n",
    "y_test_pred = model.predict(X_test_vec)\n",
    "test_acc = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "print(\"\\Testing Accuracy:\", round(test_acc * 100, 2), \"%\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_test_pred))\n",
    "\n",
    "y_train_pred = model.predict(X_train_vec)\n",
    "training_acc = accuracy_score(y_train, y_train_pred)\n",
    "print(\"\\Training Accuracy:\", round(training_acc * 100, 2), \"%\")\n",
    "print(classification_report(y_train, y_train_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22a9c504-17b9-46e0-b900-3e314ca294b0",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 25000 training samples.\n",
      "Training samples: 20000\n",
      "Validation samples: 5000\n",
      "Vectorized shape: (20000, 20000)\n",
      "\n",
      "Validation Accuracy: 89.12 %\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.89      0.89      2500\n",
      "           1       0.89      0.90      0.89      2500\n",
      "\n",
      "    accuracy                           0.89      5000\n",
      "   macro avg       0.89      0.89      0.89      5000\n",
      "weighted avg       0.89      0.89      0.89      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "# ---------------------------------------------------------------\n",
    "# 1. Load IMDB dataset (assumes local folder with pos/neg subfolders)\n",
    "# ---------------------------------------------------------------\n",
    "dataset_path = \"/Users/jensonphan/git-test/aclImdb/train\"  # adjust for your environment\n",
    "data = load_files(dataset_path, categories=['pos', 'neg'], encoding='utf-8')\n",
    "\n",
    "X_text = data.data; y = data.target\n",
    "print(\"Loaded\", len(X_text), \"training samples.\")\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 2. Train/Validation Split\n",
    "# ---------------------------------------------------------------\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_text, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"Training samples:\", len(X_train))\n",
    "print(\"Validation samples:\", len(X_val))\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 3. Vectorize with Bag-of-Words or TF–IDF\n",
    "# ---------------------------------------------------------------\n",
    "# Option 1: CountVectorizer (Bag of Words)\n",
    "# vectorizer = CountVectorizer(max_features=20001, ngram_range=(1,1))\n",
    "\n",
    "# Option 2: TF–IDF (Recommended)\n",
    "vectorizer = TfidfVectorizer(max_features=20000, ngram_range=(1,1))\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "#X_test_vec = vectorizer.fit_transform(X_)\n",
    "X_val_vec = vectorizer.transform(X_val)\n",
    "\n",
    "print(\"Vectorized shape:\", X_train_vec.shape)\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 4. Train Logistic Regression\n",
    "# ---------------------------------------------------------------\n",
    "model = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    C=1.0,              # regularization\n",
    "    solver='liblinear'  # good for sparse text data\n",
    ")\n",
    "\n",
    "model.fit(X_train_vec, y_train)\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 5. Validation Performance\n",
    "# ---------------------------------------------------------------\n",
    "y_val_pred = model.predict(X_val_vec)\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "\n",
    "print(\"\\nValidation Accuracy:\", round(val_accuracy * 100, 2), \"%\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_val, y_val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d21b7a1-b31d-40ec-817c-9e764e15e5d0",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 25000 training samples.\n",
      "Training samples: 20000\n",
      "Validation samples: 5000\n",
      "Vectorized shape: (20000, 68439)\n",
      "\n",
      "Validation Accuracy: 87.86 %\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.88      0.88      2500\n",
      "           1       0.88      0.88      0.88      2500\n",
      "\n",
      "    accuracy                           0.88      5000\n",
      "   macro avg       0.88      0.88      0.88      5000\n",
      "weighted avg       0.88      0.88      0.88      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "# ---------------------------------------------------------------\n",
    "# 1. Load IMDB dataset (assumes local folder with pos/neg subfolders)\n",
    "# ---------------------------------------------------------------\n",
    "dataset_path = \"/Users/jensonphan/git-test/aclImdb/train\"  # adjust for your environment\n",
    "data = load_files(dataset_path, categories=['pos', 'neg'], encoding='utf-8')\n",
    "\n",
    "X_text = data.data; y = data.target\n",
    "\n",
    "print(\"Loaded\", len(X_text), \"training samples.\")\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 2. Train/Validation Split\n",
    "# ---------------------------------------------------------------\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_text, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"Training samples:\", len(X_train))\n",
    "print(\"Validation samples:\", len(X_val))\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 3. Vectorize with Bag-of-Words or TF–IDF\n",
    "# ---------------------------------------------------------------\n",
    "# Option 1: CountVectorizer (Bag of Words)\n",
    "vectorizer = CountVectorizer(max_features=90000, ngram_range=(1,1))\n",
    "\n",
    "# Option 2: TF–IDF (Recommended)\n",
    "#vectorizer = TfidfVectorizer(max_features=20000, ngram_range=(1,1))\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_val_vec = vectorizer.transform(X_val)\n",
    "\n",
    "print(\"Vectorized shape:\", X_train_vec.shape)\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 4. Train Logistic Regression\n",
    "# ---------------------------------------------------------------\n",
    "model = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    C=1.0,              # regularization\n",
    "    solver='liblinear'  # good for sparse text data\n",
    ")\n",
    "\n",
    "model.fit(X_train_vec, y_train)\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 5. Validation Performance\n",
    "# ---------------------------------------------------------------\n",
    "y_val_pred = model.predict(X_val_vec)\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "\n",
    "print(\"\\nValidation Accuracy:\", round(val_accuracy * 100, 2), \"%\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_val, y_val_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4a5543c-7a39-4c4d-9788-1b1836c352c4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 25000 training samples.\n",
      "Training samples: 20000\n",
      "Validation samples: 5000\n",
      "BoW matrix shape: (20000, 20000)\n",
      "\n",
      "=== Majority Class Classifier ===\n",
      "Validation Accuracy: 50.0 %\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.50      1.00      0.67      2500\n",
      "         pos       0.00      0.00      0.00      2500\n",
      "\n",
      "    accuracy                           0.50      5000\n",
      "   macro avg       0.25      0.50      0.33      5000\n",
      "weighted avg       0.25      0.50      0.33      5000\n",
      "\n",
      "\n",
      "=== Multinomial Naive Bayes ===\n",
      "Validation Accuracy: 84.38 %\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.82      0.88      0.85      2500\n",
      "         pos       0.87      0.81      0.84      2500\n",
      "\n",
      "    accuracy                           0.84      5000\n",
      "   macro avg       0.85      0.84      0.84      5000\n",
      "weighted avg       0.85      0.84      0.84      5000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/cs178/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/opt/miniconda3/envs/cs178/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/opt/miniconda3/envs/cs178/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 1. Load IMDB dataset (train portion)\n",
    "# ---------------------------------------------------------------\n",
    "dataset_path = \"aclImdb/train\"  # adjust if needed\n",
    "\n",
    "data = load_files(\n",
    "    dataset_path,\n",
    "    categories=['pos', 'neg'],\n",
    "    encoding='utf-8'\n",
    ")\n",
    "\n",
    "X_text = data.data       # list of review strings\n",
    "y = data.target          # 0/1 labels: e.g. 0=neg, 1=pos\n",
    "\n",
    "print(\"Loaded\", len(X_text), \"training samples.\")\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 2. Train/Validation Split\n",
    "# ---------------------------------------------------------------\n",
    "X_train_text, X_val_text, y_train, y_val = train_test_split(\n",
    "    X_text, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"Training samples:\", len(X_train_text))\n",
    "print(\"Validation samples:\", len(X_val_text))\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 3. Vectorize with Bag-of-Words (word counts)\n",
    "# ---------------------------------------------------------------\n",
    "vectorizer = CountVectorizer(\n",
    "    max_features=20000,\n",
    "    ngram_range=(1, 1),  # unigrams only for now\n",
    ")\n",
    "\n",
    "X_train = vectorizer.fit_transform(X_train_text)\n",
    "X_val = vectorizer.transform(X_val_text)\n",
    "\n",
    "print(\"BoW matrix shape:\", X_train.shape)  # (n_samples, vocab_size)\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 4. Baseline 1: Majority Class Classifier\n",
    "# ---------------------------------------------------------------\n",
    "majority_clf = DummyClassifier(strategy='most_frequent')\n",
    "\n",
    "majority_clf.fit(X_train, y_train)\n",
    "\n",
    "y_val_pred_majority = majority_clf.predict(X_val)\n",
    "val_acc_majority = accuracy_score(y_val, y_val_pred_majority)\n",
    "\n",
    "print(\"\\n=== Majority Class Classifier ===\")\n",
    "print(\"Validation Accuracy:\", round(val_acc_majority * 100, 2), \"%\")\n",
    "print(classification_report(y_val, y_val_pred_majority, target_names=['neg', 'pos']))\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 5. Baseline 2: Multinomial Naive Bayes\n",
    "# ---------------------------------------------------------------\n",
    "nb_clf = MultinomialNB(alpha=1.0)  # Laplace smoothing\n",
    "\n",
    "nb_clf.fit(X_train, y_train)\n",
    "\n",
    "y_val_pred_nb = nb_clf.predict(X_val)\n",
    "val_acc_nb = accuracy_score(y_val, y_val_pred_nb)\n",
    "\n",
    "print(\"\\n=== Multinomial Naive Bayes ===\")\n",
    "print(\"Validation Accuracy:\", round(val_acc_nb * 100, 2), \"%\")\n",
    "print(classification_report(y_val, y_val_pred_nb, target_names=['neg', 'pos']))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
